{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "838cdf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a75326b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FletcAW1/Documents/repos/personal/SPAR-causal-probes/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True  # this is key\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"List 3 well-known facts about Albert Einstein.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Here are 3 well-known facts...\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, return_tensors=\"pt\", tokenize=True\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "segpz33o4m8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDX = 8   # transformer layer to probe (mid-layer; 0–15 for Llama-3.2-1B)\n",
    "BATCH_SIZE = 16  # reduce if OOM\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sa2hai8fo2h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometry of Truth: 6059 rows, label balance: {0: 3033, 1: 3026}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_file</th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>plausible_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cities_cities_conj_qa.csv</td>\n",
       "      <td>It is the case both that the city of Najafgarh...</td>\n",
       "      <td>0</td>\n",
       "      <td>Is it the case that the city of Najafgarh is i...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cities_cities_conj_qa.csv</td>\n",
       "      <td>It is the case both that the city of Cimahi is...</td>\n",
       "      <td>0</td>\n",
       "      <td>Is it the case that the city of Cimahi is in R...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 origin_file  \\\n",
       "0  cities_cities_conj_qa.csv   \n",
       "1  cities_cities_conj_qa.csv   \n",
       "\n",
       "                                           statement  label  \\\n",
       "0  It is the case both that the city of Najafgarh...      0   \n",
       "1  It is the case both that the city of Cimahi is...      0   \n",
       "\n",
       "                                            question correct_answer  \\\n",
       "0  Is it the case that the city of Najafgarh is i...             No   \n",
       "1  Is it the case that the city of Cimahi is in R...             No   \n",
       "\n",
       "  plausible_answer  \n",
       "0              Yes  \n",
       "1              Yes  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "got = pd.read_csv(\"../../data/geometry_of_truth_qa.csv\")\n",
    "print(f\"Geometry of Truth: {len(got)} rows, label balance: {got['label'].value_counts().to_dict()}\")\n",
    "got.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffqmqko8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_hidden_states(texts: list[str], layer_idx: int, batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Return (N, hidden_dim) array of last-token hidden states at layer_idx.\"\"\"\n",
    "    all_hs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, return_tensors=\"pt\", padding=True,\n",
    "            truncation=True, max_length=512\n",
    "        ).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc, output_hidden_states=True)\n",
    "        # hidden_states: tuple of (n_layers+1) tensors, each (B, T, D)\n",
    "        hs = out.hidden_states[layer_idx]              # (B, T, D)\n",
    "        last_tok = enc.attention_mask.sum(dim=1) - 1   # index of last real token\n",
    "        hs_last = hs[torch.arange(len(batch)), last_tok]  # (B, D)\n",
    "        all_hs.append(hs_last.float().cpu().numpy())\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"  {i + len(batch)}/{len(texts)}\", end=\"\\r\")\n",
    "    print()\n",
    "    return np.concatenate(all_hs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ty8uxn920f",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, StratifiedKFold, learning_curve, train_test_split\nfrom sklearn.metrics import RocCurveDisplay\nimport matplotlib.pyplot as plt\n\nprint(f\"Extracting GoT hidden states at layer {LAYER_IDX}...\")\nX_got = get_hidden_states(got[\"statement\"].tolist(), LAYER_IDX)\ny_got = got[\"label\"].values\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Per-fold train + test AUC\ncv_results = cross_validate(\n    LogisticRegression(max_iter=1000, C=0.1), X_got, y_got,\n    cv=cv, scoring=\"roc_auc\", return_train_score=True\n)\nprint(f\"\\n{'Fold':<6} {'Train AUC':>10} {'Val AUC':>10}\")\nfor i, (tr, te) in enumerate(zip(cv_results[\"train_score\"], cv_results[\"test_score\"])):\n    print(f\"{i+1:<6} {tr:>10.3f} {te:>10.3f}\")\nprint(f\"{'Mean':<6} {cv_results['train_score'].mean():>10.3f} {cv_results['test_score'].mean():>10.3f}\")\nprint(f\"{'Std':<6} {cv_results['train_score'].std():>10.3f} {cv_results['test_score'].std():>10.3f}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Learning curve\ntrain_sizes, train_scores, val_scores = learning_curve(\n    LogisticRegression(max_iter=1000, C=0.1), X_got, y_got,\n    cv=cv, scoring=\"roc_auc\", train_sizes=np.linspace(0.1, 1.0, 8), n_jobs=-1\n)\nax = axes[0]\nax.plot(train_sizes, train_scores.mean(axis=1), label=\"Train AUC\")\nax.fill_between(train_sizes,\n                train_scores.mean(axis=1) - train_scores.std(axis=1),\n                train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.2)\nax.plot(train_sizes, val_scores.mean(axis=1), label=\"Val AUC\")\nax.fill_between(train_sizes,\n                val_scores.mean(axis=1) - val_scores.std(axis=1),\n                val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.2)\nax.set_xlabel(\"Training set size\")\nax.set_ylabel(\"AUC\")\nax.set_title(\"Learning curve (GoT probe)\")\nax.legend()\n\n# ROC curve on held-out 20%\nX_tr, X_te, y_tr, y_te = train_test_split(X_got, y_got, test_size=0.2, random_state=42, stratify=y_got)\nprobe_eval = LogisticRegression(max_iter=1000, C=0.1).fit(X_tr, y_tr)\nRocCurveDisplay.from_estimator(probe_eval, X_te, y_te, ax=axes[1])\naxes[1].plot([0, 1], [0, 1], \"k--\", label=\"Random\")\naxes[1].set_title(\"ROC curve (held-out 20%)\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Refit on all GoT data for transfer evaluation\nprobe = LogisticRegression(max_iter=1000, C=0.1)\nprobe.fit(X_got, y_got)\nprint(\"\\nProbe refitted on full GoT dataset.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8v0vovxfmue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liars' Bench: 73625 rows across 8 configs\n",
      "deceptive                         False    True \n",
      "config                                          \n",
      "alpaca                           8000.0      NaN\n",
      "convincing-game                   350.0    538.0\n",
      "gender-secret                     531.0    231.0\n",
      "harm-pressure-choice             4697.0    703.0\n",
      "harm-pressure-knowledge-report   5661.0   1407.0\n",
      "insider-trading                  3757.0   2952.0\n",
      "instructed-deception            10106.0  10692.0\n",
      "soft-trigger                    12000.0  12000.0\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "\n",
    "def format_messages(messages) -> str:\n",
    "    \"\"\"Apply chat template to a full conversation including the assistant response.\"\"\"\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "lb_frames = []\n",
    "for path in sorted(glob.glob(\"../../data/liars_bench/*__test.parquet\")):\n",
    "    config = os.path.basename(path).replace(\"__test.parquet\", \"\")\n",
    "    df = pd.read_parquet(path)[[\"messages\", \"deceptive\"]].copy()\n",
    "    df[\"config\"] = config\n",
    "    lb_frames.append(df)\n",
    "\n",
    "lb = pd.concat(lb_frames, ignore_index=True)\n",
    "lb[\"text\"] = lb[\"messages\"].apply(format_messages)\n",
    "\n",
    "print(f\"Liars' Bench: {len(lb)} rows across {lb['config'].nunique()} configs\")\n",
    "print(lb.groupby(\"config\")[\"deceptive\"].value_counts().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n0xexr3nbg8",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import roc_auc_score, accuracy_score\n\n# GoT label=1 means TRUE; LB deceptive=True means LYING → use class-0 probability\nresults = []\nfor config, grp in lb.groupby(\"config\"):\n    grp = grp.sample(min(500, len(grp)), random_state=42)\n    print(f\"Extracting: {config} ({len(grp)} rows)...\")\n    X = get_hidden_states(grp[\"text\"].tolist(), LAYER_IDX)\n    y = grp[\"deceptive\"].astype(int).values\n    # class 0 = \"true\" in GoT ≈ \"deceptive\" in LB\n    deception_score = probe.predict_proba(X)[:, 0]\n    try:\n        auc = roc_auc_score(y, deception_score)\n    except ValueError:\n        auc = float(\"nan\")  # single class (e.g. alpaca is all non-deceptive)\n    preds = (deception_score > 0.5).astype(int)\n    results.append({\n        \"config\": config,\n        \"n\": len(grp),\n        \"frac_deceptive\": y.mean().round(3),\n        \"AUC\": auc,\n        \"Acc\": accuracy_score(y, preds),\n    })\n\nresults_df = pd.DataFrame(results).set_index(\"config\")\nprint(\"\\n--- Probe transfer results (trained on GoT, tested on Liars' Bench) ---\")\nprint(results_df.to_string(float_format=\"{:.3f}\".format))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spar-causal-probes (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}