{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# DCT Experiment — Local Mac Version\n",
    "\n",
    "Adapted from `dct_experiment.ipynb` to run locally on Apple Silicon (MPS) or CPU.\n",
    "\n",
    "**Changes from GPU version:**\n",
    "- Model: `Qwen/Qwen1.5-0.5B-Chat` (0.5B params, same architecture as 7B so all DCT code is compatible)\n",
    "- Device: MPS (Apple Silicon) with CPU fallback\n",
    "- Reduced hyperparameters: fewer factors, smaller projection dim, tighter layer window\n",
    "- No hardcoded `.cuda()` calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # fallback to CPU for unsupported MPS ops (e.g. linalg_qr)\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Drop any variables from a previous run\n",
    "for _var in [\"model\", \"tokenizer\", \"sliced_model\", \"delta_acts_single\", \"delta_acts\",\n",
    "             \"steering_calibrator\", \"exp_dct\", \"X\", \"Y\", \"U\", \"V\", \"hidden_states\"]:\n",
    "    if _var in dir():\n",
    "        del globals()[_var]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Detect best available device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1113f69f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add local repo root so we can import dct.py\n",
    "REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "sys.modules.pop(\"dct\", None)  # evict cached import\n",
    "import dct\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch import vmap\n",
    "import torch\n",
    "\n",
    "torch.set_default_device(DEVICE)\n",
    "torch.set_default_dtype(torch.float32)  # float32 for MPS/CPU compatibility\n",
    "torch.manual_seed(325)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-heading",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "`Qwen/Qwen1.5-0.5B-Chat` has **24 layers** and `d_model=1024`.\n",
    "Source layer 5 → target layer 12 covers the middle third of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME      = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "TOKENIZER_NAME  = MODEL_NAME\n",
    "\n",
    "INPUT_SCALE = None          # set to None to use calibration\n",
    "\n",
    "NUM_SAMPLES          = 1\n",
    "FORWARD_BATCH_SIZE   = 1\n",
    "BACKWARD_BATCH_SIZE  = 1\n",
    "MAX_SEQ_LEN          = 27\n",
    "\n",
    "CALIBRATION_SAMPLE_SIZE        = 10   # fewer random directions for calibration\n",
    "CALIBRATION_PROMPT_SAMPLE_SIZE = 1\n",
    "\n",
    "DIM_OUTPUT_PROJECTION = 8    # reduced from 32\n",
    "\n",
    "NUM_ITERS           = 5      # reduced for local testing\n",
    "NUM_FACTORS         = 32     # reduced from 512\n",
    "FACTOR_BATCH_SIZE   = 4      # reduced from 16\n",
    "\n",
    "SOURCE_LAYER_IDX = 5         # ~20% through 24-layer model\n",
    "TARGET_LAYER_IDX = 12        # ~50% through\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant\"\n",
    "TOKEN_IDXS    = slice(-3, None)\n",
    "NUM_EVAL      = 16           # reduced for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fletcaw1/Documents/Personal/personal-repos/SPAR-causal-probes/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1\n",
      "Test examples:     32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "response = requests.get(url)\n",
    "dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "instructions = dataset['goal'].tolist()\n",
    "\n",
    "import random\n",
    "random.seed(325)\n",
    "if NUM_SAMPLES == 1:\n",
    "    instructions = [\"Tell me how to make a bomb.\"] + instructions\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TOKENIZER_NAME, trust_remote_code=True,\n",
    "    padding_side=\"left\", truncation_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "chat_init = ([{'content': SYSTEM_PROMPT, 'role': 'system'}]\n",
    "             if SYSTEM_PROMPT is not None else [])\n",
    "chats = [chat_init + [{'content': c, 'role': 'user'}]\n",
    "         for c in instructions[:NUM_SAMPLES]]\n",
    "EXAMPLES = [tokenizer.apply_chat_template(\n",
    "    chat, add_special_tokens=False, tokenize=False, add_generation_prompt=True)\n",
    "    for chat in chats]\n",
    "\n",
    "test_chats = [chat_init + [{'content': c, 'role': 'user'}]\n",
    "              for c in instructions[-32:]]\n",
    "TEST_EXAMPLES = [tokenizer.apply_chat_template(\n",
    "    chat, add_special_tokens=False, tokenize=False, add_generation_prompt=True)\n",
    "    for chat in test_chats]\n",
    "\n",
    "print(f\"Training examples: {len(EXAMPLES)}\")\n",
    "print(f\"Test examples:     {len(TEST_EXAMPLES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen1.5-0.5B-Chat\n",
      "Num layers:   24\n",
      "d_model:      1024\n",
      "Device:       mps:0\n",
      "CPU times: user 330 ms, sys: 1.48 s, total: 1.81 s\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    _attn_implementation=\"eager\",  # required for torch.func\n",
    "    torch_dtype=torch.float32,\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Num layers:   {model.config.num_hidden_layers}\")\n",
    "print(f\"d_model:      {model.config.hidden_size}\")\n",
    "print(f\"Device:       {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sanity-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlicedModel sanity check passed.\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: verify SlicedModel round-trips correctly\n",
    "model_inputs = tokenizer(\n",
    "    [\"tell me how to make a bomb\"],\n",
    "    return_tensors=\"pt\", truncation=True\n",
    ").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden_states = model(\n",
    "        model_inputs[\"input_ids\"], output_hidden_states=True\n",
    "    ).hidden_states\n",
    "\n",
    "sliced_test = dct.SlicedModel(\n",
    "    model, start_layer=3, end_layer=5, layers_name=\"model.layers\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    out = sliced_test(hidden_states[3])\n",
    "    assert torch.allclose(out, hidden_states[5], atol=1e-4), \\\n",
    "        f\"SlicedModel mismatch! max_diff={( out - hidden_states[5]).abs().max()}\"\n",
    "print(\"SlicedModel sanity check passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sliced-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_model = dct.SlicedModel(\n",
    "    model,\n",
    "    start_layer=SOURCE_LAYER_IDX,\n",
    "    end_layer=TARGET_LAYER_IDX,\n",
    "    layers_name=\"model.layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collect-activations",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1, 27, 1024]), Y shape: torch.Size([1, 27, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_model = model.config.hidden_size\n",
    "\n",
    "X = torch.zeros(NUM_SAMPLES, MAX_SEQ_LEN, d_model, device=\"cpu\", dtype=model.dtype)\n",
    "Y = torch.zeros(NUM_SAMPLES, MAX_SEQ_LEN, d_model, device=\"cpu\", dtype=model.dtype)\n",
    "\n",
    "for t in tqdm(range(0, NUM_SAMPLES, FORWARD_BATCH_SIZE)):\n",
    "    with torch.no_grad():\n",
    "        model_inputs = tokenizer(\n",
    "            EXAMPLES[t:t + FORWARD_BATCH_SIZE],\n",
    "            return_tensors=\"pt\", truncation=True,\n",
    "            padding=\"max_length\", max_length=MAX_SEQ_LEN\n",
    "        ).to(DEVICE)\n",
    "        hidden_states = model(\n",
    "            model_inputs[\"input_ids\"], output_hidden_states=True\n",
    "        ).hidden_states\n",
    "        h_source        = hidden_states[SOURCE_LAYER_IDX]\n",
    "        unsteered_target = sliced_model(h_source)\n",
    "\n",
    "        X[t:t + FORWARD_BATCH_SIZE] = h_source.cpu()\n",
    "        Y[t:t + FORWARD_BATCH_SIZE] = unsteered_target.cpu()\n",
    "\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "delta-acts",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_acts_single = dct.DeltaActivations(\n",
    "    sliced_model, target_position_indices=TOKEN_IDXS\n",
    ")\n",
    "delta_acts = vmap(\n",
    "    delta_acts_single, in_dims=(1, None, None), out_dims=2,\n",
    "    chunk_size=FACTOR_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "calibrate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_SCALE: 3.068743559156747\n",
      "CPU times: user 1.14 s, sys: 505 ms, total: 1.64 s\n",
      "Wall time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "steering_calibrator = dct.SteeringCalibrator(target_ratio=0.5)\n",
    "if INPUT_SCALE is None:\n",
    "    # calibrate() moves batches to delta_acts_single.device internally\n",
    "    INPUT_SCALE = steering_calibrator.calibrate(\n",
    "        delta_acts_single,\n",
    "        X, Y,\n",
    "        factor_batch_size=FACTOR_BATCH_SIZE,\n",
    "        calibration_sample_size=CALIBRATION_SAMPLE_SIZE,\n",
    "    )\n",
    "print(f\"INPUT_SCALE: {INPUT_SCALE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fit-dct",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing jacobian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing SVD of jacobian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/fletcaw1/Documents/Personal/personal-repos/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/utils/_device.py:109: UserWarning: The operator 'aten::linalg_svd' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:15.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing output directions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: torch.Size([1024, 32]), V shape: torch.Size([1024, 32])\n",
      "CPU times: user 1.45 s, sys: 304 ms, total: 1.75 s\n",
      "Wall time: 3.39 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "exp_dct = dct.ExponentialDCT(num_factors=NUM_FACTORS)\n",
    "U, V = exp_dct.fit(\n",
    "    delta_acts_single,\n",
    "    X, Y,\n",
    "    batch_size=BACKWARD_BATCH_SIZE,\n",
    "    factor_batch_size=FACTOR_BATCH_SIZE,\n",
    "    init=\"jacobian\",\n",
    "    d_proj=DIM_OUTPUT_PROJECTION,\n",
    "    input_scale=INPUT_SCALE,\n",
    "    max_iters=NUM_ITERS,\n",
    "    beta=1.0,\n",
    ")\n",
    "print(f\"U shape: {U.shape}, V shape: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results",
   "metadata": {},
   "outputs": [],
   "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 3))\nplt.plot(exp_dct.objective_values, marker=\"o\", markersize=4)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Objective (fdot)\")\nplt.title(\"ExponentialDCT training objective\")\nplt.tight_layout()\nplt.savefig(\"dct_local_objective.png\", dpi=100)\nplt.show()\nprint(\"Objective values:\", exp_dct.objective_values)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spar-causal-probes (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}