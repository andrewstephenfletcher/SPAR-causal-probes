{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# DCT Experiment — Local Mac Version\n",
    "\n",
    "Adapted from `dct_experiment.ipynb` to run locally on Apple Silicon (MPS) or CPU.\n",
    "\n",
    "**Changes from GPU version:**\n",
    "- Model: `Qwen/Qwen1.5-0.5B-Chat` (0.5B params, same architecture as 7B so all DCT code is compatible)\n",
    "- Device: MPS (Apple Silicon) with CPU fallback\n",
    "- Reduced hyperparameters: fewer factors, smaller projection dim, tighter layer window\n",
    "- No hardcoded `.cuda()` calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # fallback to CPU for unsupported MPS ops (e.g. linalg_qr)\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Drop any variables from a previous run\n",
    "for _var in [\"model\", \"tokenizer\", \"sliced_model\", \"delta_acts_single\", \"delta_acts\",\n",
    "             \"steering_calibrator\", \"exp_dct\", \"X\", \"Y\", \"U\", \"V\", \"hidden_states\"]:\n",
    "    if _var in dir():\n",
    "        del globals()[_var]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Detect best available device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd305dcd330>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add local repo root so we can import dct.py\n",
    "REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "sys.modules.pop(\"dct\", None)  # evict cached import\n",
    "import dct\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch import vmap\n",
    "import torch\n",
    "\n",
    "torch.set_default_device(DEVICE)\n",
    "# bfloat16 on CUDA halves model memory (~14 GB vs ~28 GB for 7B); float32 for MPS/CPU\n",
    "DTYPE = torch.bfloat16 if DEVICE == \"cuda\" else torch.float32\n",
    "torch.set_default_dtype(DTYPE)\n",
    "torch.manual_seed(325)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-heading",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "`Qwen/Qwen1.5-0.5B-Chat` has **24 layers** and `d_model=1024`.\n",
    "Source layer 5 → target layer 12 covers the middle third of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME      = \"jane-street/dormant-model-warmup\"\n",
    "TOKENIZER_NAME  = MODEL_NAME\n",
    "\n",
    "INPUT_SCALE = None          # set to None to use calibration\n",
    "\n",
    "NUM_SAMPLES          = 1\n",
    "FORWARD_BATCH_SIZE   = 1\n",
    "BACKWARD_BATCH_SIZE  = 1\n",
    "MAX_SEQ_LEN          = 27\n",
    "\n",
    "CALIBRATION_SAMPLE_SIZE        = 10   # fewer random directions for calibration\n",
    "CALIBRATION_PROMPT_SAMPLE_SIZE = 1\n",
    "\n",
    "DIM_OUTPUT_PROJECTION = 32    # reduced from 32\n",
    "\n",
    "NUM_ITERS           = 10      # reduced for local testing\n",
    "NUM_FACTORS         = 32     # reduced from 512\n",
    "FACTOR_BATCH_SIZE   = 4      # reduced from 16\n",
    "\n",
    "SOURCE_LAYER_IDX = 8         # ~20% through 24-layer model\n",
    "TARGET_LAYER_IDX = 20        # ~50% through\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant\"\n",
    "TOKEN_IDXS    = slice(-3, None)\n",
    "NUM_EVAL      = 16           # reduced for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1\n",
      "Test examples:     32\n",
      "Example prompt:\\n<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Concerning Hobbits,<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TOKENIZER_NAME, trust_remote_code=True,\n",
    "    padding_side=\"left\", truncation_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# jane-street/dormant-model-warmup is Qwen-based but its tokenizer\n",
    "# doesn't ship a chat_template — set the standard Qwen ChatML template.\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{{'<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>' + '\\\\n'}}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\\n' }}{% endif %}\"\n",
    "    )\n",
    "\n",
    "instructions = [\"Concerning Hobbit,\"] * NUM_SAMPLES\n",
    "\n",
    "chat_init = ([{'content': SYSTEM_PROMPT, 'role': 'system'}]\n",
    "             if SYSTEM_PROMPT is not None else [])\n",
    "chats = [chat_init + [{'content': c, 'role': 'user'}] for c in instructions]\n",
    "EXAMPLES = [tokenizer.apply_chat_template(\n",
    "    chat, add_special_tokens=False, tokenize=False, add_generation_prompt=True)\n",
    "    for chat in chats]\n",
    "TEST_EXAMPLES = EXAMPLES  # reuse for eval with n=1\n",
    "\n",
    "print(f\"Training examples: {len(EXAMPLES)}\")\n",
    "print(f\"Example prompt:\\n{EXAMPLES[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: jane-street/dormant-model-warmup\n",
      "Num layers:   28\n",
      "d_model:      3584\n",
      "Device:       cuda:0\n",
      "CPU times: user 4min 13s, sys: 2min 35s, total: 6min 48s\n",
      "Wall time: 17.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    _attn_implementation=\"eager\",  # required for torch.func\n",
    "    torch_dtype=DTYPE,\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Num layers:   {model.config.num_hidden_layers}\")\n",
    "print(f\"d_model:      {model.config.hidden_size}\")\n",
    "print(f\"dtype:        {model.dtype}\")\n",
    "print(f\"Device:       {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sanity-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlicedModel sanity check passed.\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: verify SlicedModel round-trips correctly\n",
    "model_inputs = tokenizer(\n",
    "    [\"Concerning Hobbits,\"],\n",
    "    return_tensors=\"pt\", truncation=True\n",
    ").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden_states = model(\n",
    "        model_inputs[\"input_ids\"], output_hidden_states=True\n",
    "    ).hidden_states\n",
    "\n",
    "sliced_test = dct.SlicedModel(\n",
    "    model, start_layer=3, end_layer=5, layers_name=\"model.layers\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    out = sliced_test(hidden_states[3])\n",
    "    assert torch.allclose(out, hidden_states[5], atol=1e-4), \\\n",
    "        f\"SlicedModel mismatch! max_diff={( out - hidden_states[5]).abs().max()}\"\n",
    "print(\"SlicedModel sanity check passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sliced-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_model = dct.SlicedModel(\n",
    "    model,\n",
    "    start_layer=SOURCE_LAYER_IDX,\n",
    "    end_layer=TARGET_LAYER_IDX,\n",
    "    layers_name=\"model.layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collect-activations",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1, 27, 3584]), Y shape: torch.Size([1, 27, 3584])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_model = model.config.hidden_size\n",
    "\n",
    "X = torch.zeros(NUM_SAMPLES, MAX_SEQ_LEN, d_model, device=\"cpu\", dtype=model.dtype)\n",
    "Y = torch.zeros(NUM_SAMPLES, MAX_SEQ_LEN, d_model, device=\"cpu\", dtype=model.dtype)\n",
    "\n",
    "for t in tqdm(range(0, NUM_SAMPLES, FORWARD_BATCH_SIZE)):\n",
    "    with torch.no_grad():\n",
    "        model_inputs = tokenizer(\n",
    "            EXAMPLES[t:t + FORWARD_BATCH_SIZE],\n",
    "            return_tensors=\"pt\", truncation=True,\n",
    "            padding=\"max_length\", max_length=MAX_SEQ_LEN\n",
    "        ).to(DEVICE)\n",
    "        hidden_states = model(\n",
    "            model_inputs[\"input_ids\"], output_hidden_states=True\n",
    "        ).hidden_states\n",
    "        h_source        = hidden_states[SOURCE_LAYER_IDX]\n",
    "        unsteered_target = sliced_model(h_source)\n",
    "\n",
    "        X[t:t + FORWARD_BATCH_SIZE] = h_source.cpu()\n",
    "        Y[t:t + FORWARD_BATCH_SIZE] = unsteered_target.cpu()\n",
    "\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "delta-acts",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_acts_single = dct.DeltaActivations(\n",
    "    sliced_model, target_position_indices=TOKEN_IDXS\n",
    ")\n",
    "delta_acts = vmap(\n",
    "    delta_acts_single, in_dims=(1, None, None), out_dims=2,\n",
    "    chunk_size=FACTOR_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "calibrate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 859 ms, total: 2.31 s\n",
      "Wall time: 7.07 s\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.12 GiB. GPU 0 has a total capacity of 31.36 GiB of which 1.86 GiB is free. Including non-PyTorch memory, this process has 29.49 GiB memory in use. Of the allocated memory 28.69 GiB is allocated by PyTorch, and 228.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msteering_calibrator = dct.SteeringCalibrator(target_ratio=0.5)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mif INPUT_SCALE is None:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # calibrate() moves batches to delta_acts_single.device internally\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    INPUT_SCALE = steering_calibrator.calibrate(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        delta_acts_single,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        X, Y,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        factor_batch_size=FACTOR_BATCH_SIZE,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        calibration_sample_size=CALIBRATION_SAMPLE_SIZE,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mINPUT_SCALE: \u001b[39;49m\u001b[38;5;132;43;01m{INPUT_SCALE}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/IPython/core/magics/execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/IPython/core/magics/execution.py:1411\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1409\u001b[39m st = clock2()\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:4\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/dct.py:150\u001b[39m, in \u001b[36mSteeringCalibrator.calibrate\u001b[39m\u001b[34m(self, delta_acts_single, X, Y, batch_size, calibration_sample_size, factor_batch_size)\u001b[39m\n\u001b[32m    148\u001b[39m         x = X[b:b+batch_size,:,:].to(delta_acts_single.device)\n\u001b[32m    149\u001b[39m         y = Y[b:b+batch_size,:,:].to(delta_acts_single.device)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         U_cal_batch = \u001b[43mjvp_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_cal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         U_cal_avg.update(U_cal_batch)\n\u001b[32m    152\u001b[39m U_cal = U_cal_avg.get_mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/_functorch/apis.py:208\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/_functorch/vmap.py:272\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    269\u001b[39m     chunks_flat_args = _get_chunked_inputs(\n\u001b[32m    270\u001b[39m         flat_args, flat_in_dims, batch_size, chunk_size\n\u001b[32m    271\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chunked_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunks_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[32m    284\u001b[39m     func,\n\u001b[32m    285\u001b[39m     batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m     **kwargs,\n\u001b[32m    292\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/_functorch/vmap.py:384\u001b[39m, in \u001b[36m_chunked_vmap\u001b[39m\u001b[34m(func, flat_in_dims, chunks_flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    382\u001b[39m         torch.set_rng_state(rs)\n\u001b[32m    383\u001b[39m     chunks_output.append(\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m         \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m     )\n\u001b[32m    396\u001b[39m flat_output_chunks, arg_spec = _flatten_chunks_output(chunks_output)\n\u001b[32m    398\u001b[39m \u001b[38;5;66;03m# chunked output tensors are held by both `flat_output_chunks` and `chunks_output`.\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# eagerly remove the reference from `chunks_output`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/_functorch/vmap.py:433\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    430\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    431\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    432\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/dct.py:143\u001b[39m, in \u001b[36mSteeringCalibrator.calibrate.<locals>.<lambda>\u001b[39m\u001b[34m(v, X, Y)\u001b[39m\n\u001b[32m    141\u001b[39m     _, jvp_out = jvp(\u001b[38;5;28;01mlambda\u001b[39;00m _v: delta_acts_single(_v,X,Y), (v0,), (v,))\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m jvp_batch = vmap(\u001b[38;5;28;01mlambda\u001b[39;00m v, X, Y: \u001b[43mjvp_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m, in_dims=(\u001b[32m1\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m), out_dims=(\u001b[32m2\u001b[39m), chunk_size=factor_batch_size)\n\u001b[32m    145\u001b[39m U_cal_avg = StreamingAverage()\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/dct.py:141\u001b[39m, in \u001b[36mSteeringCalibrator.calibrate.<locals>.jvp_single\u001b[39m\u001b[34m(v, X, Y)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjvp_single\u001b[39m(v,X,Y):\n\u001b[32m    140\u001b[39m     v0 = torch.zeros_like(v)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     _, jvp_out = \u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_v\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_acts_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mv0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1043\u001b[39m, in \u001b[36mjvp\u001b[39m\u001b[34m(func, primals, tangents, strict, has_aux)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;129m@exposed_in\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtorch.func\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjvp\u001b[39m(\n\u001b[32m    986\u001b[39m     func: Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    991\u001b[39m     has_aux: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    992\u001b[39m ):\n\u001b[32m    993\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[33;03m    Standing for the Jacobian-vector product, returns a tuple containing\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[33;03m    the output of `func(*primals)` and the \"Jacobian of ``func`` evaluated at\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1040\u001b[39m \n\u001b[32m   1041\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_aux\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1102\u001b[39m, in \u001b[36m_jvp_with_argnums\u001b[39m\u001b[34m(func, primals, tangents, argnums, strict, has_aux)\u001b[39m\n\u001b[32m   1100\u001b[39m     primals = _wrap_all_tensors(primals, level)\n\u001b[32m   1101\u001b[39m     duals = _replace_args(primals, duals, argnums)\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m result_duals = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) == \u001b[32m2\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/dct.py:141\u001b[39m, in \u001b[36mSteeringCalibrator.calibrate.<locals>.jvp_single.<locals>.<lambda>\u001b[39m\u001b[34m(_v)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjvp_single\u001b[39m(v,X,Y):\n\u001b[32m    140\u001b[39m     v0 = torch.zeros_like(v)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     _, jvp_out = jvp(\u001b[38;5;28;01mlambda\u001b[39;00m _v: \u001b[43mdelta_acts_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m, (v0,), (v,))\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/dct.py:74\u001b[39m, in \u001b[36mDeltaActivations.forward\u001b[39m\u001b[34m(self, theta, x, y)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, theta, x, y):\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    computes average delta in target layer activations as a \u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    function of bias theta\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     delta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msliced_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m+\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m - y \u001b[38;5;66;03m# batch_size x seq_len x d_model\u001b[39;00m\n\u001b[32m     75\u001b[39m     delta = delta[:, \u001b[38;5;28mself\u001b[39m.target_position_indices, :]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m delta.mean(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/dct.py:57\u001b[39m, in \u001b[36mSlicedModel.forward\u001b[39m\u001b[34m(self, h)\u001b[39m\n\u001b[32m     54\u001b[39m     rgetattr(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.layers_name)[i].self_attn.layer_idx = i\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# actually run the forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.hidden_states[\u001b[38;5;28mself\u001b[39m.end_layer-\u001b[38;5;28mself\u001b[39m.start_layer]\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# reset model to un-mutated state\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:1179\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1178\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1181\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SPAR-causal-probes/.venv/lib/python3.12/site-packages/torch/utils/_device.py:109\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 8.12 GiB. GPU 0 has a total capacity of 31.36 GiB of which 1.86 GiB is free. Including non-PyTorch memory, this process has 29.49 GiB memory in use. Of the allocated memory 28.69 GiB is allocated by PyTorch, and 228.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "steering_calibrator = dct.SteeringCalibrator(target_ratio=0.5)\n",
    "if INPUT_SCALE is None:\n",
    "    # calibrate() moves batches to delta_acts_single.device internally\n",
    "    INPUT_SCALE = steering_calibrator.calibrate(\n",
    "        delta_acts_single,\n",
    "        X, Y,\n",
    "        factor_batch_size=FACTOR_BATCH_SIZE,\n",
    "        calibration_sample_size=CALIBRATION_SAMPLE_SIZE,\n",
    "    )\n",
    "print(f\"INPUT_SCALE: {INPUT_SCALE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-dct",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "exp_dct = dct.ExponentialDCT(num_factors=NUM_FACTORS)\n",
    "U, V = exp_dct.fit(\n",
    "    delta_acts_single,\n",
    "    X, Y,\n",
    "    batch_size=BACKWARD_BATCH_SIZE,\n",
    "    factor_batch_size=FACTOR_BATCH_SIZE,\n",
    "    init=\"jacobian\",\n",
    "    d_proj=DIM_OUTPUT_PROJECTION,\n",
    "    input_scale=INPUT_SCALE,\n",
    "    max_iters=NUM_ITERS,\n",
    "    beta=1.0,\n",
    ")\n",
    "print(f\"U shape: {U.shape}, V shape: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(exp_dct.objective_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff736c51",
   "metadata": {},
   "outputs": [],
   "source": "with torch.no_grad():\n    simu = (U.t() @ U)\n    simu = simu[torch.triu(torch.ones_like(simu), diagonal=1).bool()]\nimport seaborn as sns\nsns.displot(simu.float().cpu())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2535288",
   "metadata": {},
   "outputs": [],
   "source": "with torch.no_grad():\n    simv = (V.t() @ V)\n    simv = simv[torch.triu(torch.ones_like(simv), diagonal=1).bool()]\nimport seaborn as sns\nsns.displot(simv.float().cpu())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee99900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(EXAMPLES[:1], return_tensors=\"pt\").to(DEVICE)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=64, do_sample=False)\n",
    "completion = tokenizer.batch_decode(generated_ids,\n",
    "                       skip_special_tokens=True)[0]\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea09200",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_to_end = dct.SlicedModel(model, start_layer=SOURCE_LAYER_IDX, end_layer=model.config.num_hidden_layers-1,\n",
    "                               layers_name=\"model.layers\")\n",
    "delta_acts_end_single = dct.DeltaActivations(slice_to_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f1cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SORRY_TOKEN = tokenizer.encode(\"Sorry\", add_special_tokens=False)[0]\n",
    "SURE_TOKEN = tokenizer.encode(\"Sure\", add_special_tokens=False)[0]\n",
    "with torch.no_grad():\n",
    "    target_vec = model.lm_head.weight.data[SURE_TOKEN,:] - model.lm_head.weight.data[SORRY_TOKEN,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8748fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = exp_dct.rank(delta_acts_end_single, X, Y, target_vec=target_vec,\n",
    "                               batch_size=FORWARD_BATCH_SIZE, factor_batch_size=FACTOR_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f24c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(scores.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_editor = dct.ModelEditor(model, layers_name=\"model.layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd85ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EVAL = 64\n",
    "MAX_NEW_TOKENS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c963a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_rand = torch.nn.functional.normalize(torch.randn(d_model, NUM_EVAL), dim=0)\n",
    "completions = []\n",
    "prompt = EXAMPLES[0]\n",
    "for i in tqdm(range(NUM_EVAL)):\n",
    "    model_editor.restore()\n",
    "    model_editor.steer(INPUT_SCALE*V_rand[:,i], SOURCE_LAYER_IDX)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "    completion = tokenizer.batch_decode(generated_ids,\n",
    "                           skip_special_tokens=True)[0]\n",
    "    completions.append(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_EVAL):\n",
    "    print(\"====Random Vector %d, Positive :=========\\n\" % i)\n",
    "    print(completions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 128\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6876cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_editor.restore()\n",
    "completions = []\n",
    "prompt = EXAMPLES[0]\n",
    "\n",
    "indices = list(range(min(NUM_EVAL, NUM_FACTORS)))\n",
    "\n",
    "for factor_idx in tqdm(indices):\n",
    "    model_editor.restore()\n",
    "    model_editor.steer(INPUT_SCALE*V[:,factor_idx], SOURCE_LAYER_IDX)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "    completion = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    completions.append(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c623d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indices)):\n",
    "    print(\"====Steered by vector %d=========\\n\" % i)\n",
    "    print(completions[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spar-causal-probes (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}